{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "text-classification-with-python-and-keras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ftk1000/colab_demos/blob/master/text_classification_with_python_and_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b5d4c3d412ce81cc9869656bb879240efccc13cf",
        "id": "ll4bpquDzP9p",
        "colab_type": "text"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "* Data Set\n",
        "* Defining a Baseline Model\n",
        "* Introduction to Deep Neural Networks\n",
        "    * Introducing Keras\n",
        "    * First Keras Model\n",
        "* What Is a Word Embedding?\n",
        "     * One-Hot Encoding\n",
        "     * Word Embeddings\n",
        "     * Keras Embedding Layer\n",
        "* Convolutional Neural Networks (CNN)\n",
        "*  Hyperparameters Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1403135d321b059084d6416c5fadcd8d79bd5d49",
        "id": "I-gmhZYtzP9r",
        "colab_type": "text"
      },
      "source": [
        "### Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "zYQaeCEgzP9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import os\n",
        "# print(os.listdir(\"../input\"))\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc4J9elczs6_",
        "colab_type": "text"
      },
      "source": [
        "# PULL DATA FROM KAGGLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNU_gGgqzoXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f54ab6cf-e6c5-4207-ad90-0faa1afb1000"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arbr1_Piz3Mi",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "08fd29b4-dd80-428b-a5e6-7d8c61006ccc"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6225f2ca-f800-45b1-a308-6bd62f2d1d88\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6225f2ca-f800-45b1-a308-6bd62f2d1d88\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle_20200901.json to kaggle_20200901.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle_20200901.json': b'{\"username\":\"zurman\",\"key\":\"09a91dfb7e3922f4f752e77ff3733b27\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_b3f9lhz_my",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle_20200901.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2qr8-4c0OVb",
        "colab_type": "text"
      },
      "source": [
        "# DOWNLOAD textclassificationdb.zip\n",
        "# !kaggle datasets download -d luishpinto/textclassificationdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F9fTV_E0L2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c56f19b7-0300-404c-9fdd-5077d49b0c31"
      },
      "source": [
        "!kaggle datasets download -d luishpinto/textclassificationdb"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading textclassificationdb.zip to /content\n",
            "\r  0% 0.00/80.3k [00:00<?, ?B/s]\n",
            "\r100% 80.3k/80.3k [00:00<00:00, 25.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x7GoiQz1mwv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "993cbd17-ccb8-4fc3-aae2-3155e44337c5"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"textclassificationdb.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LDlgkkuMzP9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "afb614e0-d9ae-4250-ab39-a6db1f96e412"
      },
      "source": [
        "!head -10 amazon_cells_labelled.txt\n",
        "!pwd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "So there is no way for me to plug it in here in the US unless I go by a converter.\t0\n",
            "Good case, Excellent value.\t1\n",
            "Great for the jawbone.\t1\n",
            "Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\t0\n",
            "The mic is great.\t1\n",
            "I have to jiggle the plug to get it to line up right to get decent volume.\t0\n",
            "If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.\t0\n",
            "If you are Razr owner...you must have this!\t1\n",
            "Needless to say, I wasted my money.\t0\n",
            "What a waste of money and time!.\t0\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PCZqPECkzP93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3b5229c5-fc5f-4986-bc3b-8adb07cd0abf"
      },
      "source": [
        "#!ls -la ../input/sentiment\\ labelled\\ sentences \n",
        "# !ls -la /kaggle/input/sentiment\\ labelled\\ sentences \n",
        "!ls -l"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 296\n",
            "-rw-r--r-- 1 root root 58226 Sep  1 22:43 amazon_cells_labelled.txt\n",
            "-rw-r--r-- 1 root root 85285 Sep  1 22:43 imdb_labelled.txt\n",
            "-rw-r--r-- 1 root root    62 Sep  1 22:35 kaggle_20200901.json\n",
            "drwxr-xr-x 1 root root  4096 Aug 27 16:39 sample_data\n",
            "-rw-r--r-- 1 root root 82239 Sep  1 22:42 textclassificationdb.zip\n",
            "-rw-r--r-- 1 root root 61320 Sep  1 22:43 yelp_labelled.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HTDy8PxGzP98",
        "colab_type": "code",
        "colab": {},
        "outputId": "e2aee1d8-868a-48df-bbad-f494c37fe59e"
      },
      "source": [
        "# !cp /kaggle/input/sentiment\\ labelled\\ sentences/*.txt www.kaggle.com/zurman/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: target 'www.kaggle.com/zurman/' is not a directory\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "95d949291c93842c872d9dcee6924ff56024cf8a",
        "id": "YsrSMoT4zP-B",
        "colab_type": "text"
      },
      "source": [
        "Extract the folder into a data folder and go ahead and load the data with Pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "vuF_IG31zP-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/zurman/text-classification-with-python-and-keras/edit\n",
        "import pandas as pd\n",
        "\n",
        "filepath_dict = {'yelp':   'yelp_labelled.txt', #../input/sentiment labelled sentences/sentiment labelled sentences/\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "# df_list"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "h8TL4eg9zP-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BkiylujJzP-N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "9554f8a9-ae26-435a-db9b-b3b0306709bb"
      },
      "source": [
        "df_list"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[                                              sentence  label source\n",
              " 0                             Wow... Loved this place.      1   yelp\n",
              " 1                                   Crust is not good.      0   yelp\n",
              " 2            Not tasty and the texture was just nasty.      0   yelp\n",
              " 3    Stopped by during the late May bank holiday of...      1   yelp\n",
              " 4    The selection on the menu was great and so wer...      1   yelp\n",
              " ..                                                 ...    ...    ...\n",
              " 995  I think food should have flavor and texture an...      0   yelp\n",
              " 996                           Appetite instantly gone.      0   yelp\n",
              " 997  Overall I was not impressed and would not go b...      0   yelp\n",
              " 998  The whole experience was underwhelming, and I ...      0   yelp\n",
              " 999  Then, as if I hadn't wasted enough of my life ...      0   yelp\n",
              " \n",
              " [1000 rows x 3 columns],\n",
              "                                               sentence  label  source\n",
              " 0    So there is no way for me to plug it in here i...      0  amazon\n",
              " 1                          Good case, Excellent value.      1  amazon\n",
              " 2                               Great for the jawbone.      1  amazon\n",
              " 3    Tied to charger for conversations lasting more...      0  amazon\n",
              " 4                                    The mic is great.      1  amazon\n",
              " ..                                                 ...    ...     ...\n",
              " 995  The screen does get smudged easily because it ...      0  amazon\n",
              " 996  What a piece of junk.. I lose more calls on th...      0  amazon\n",
              " 997                       Item Does Not Match Picture.      0  amazon\n",
              " 998  The only thing that disappoint me is the infra...      0  amazon\n",
              " 999  You can not answer calls with the unit, never ...      0  amazon\n",
              " \n",
              " [1000 rows x 3 columns],\n",
              "                                               sentence  label source\n",
              " 0    A very, very, very slow-moving, aimless movie ...      0   imdb\n",
              " 1    Not sure who was more lost - the flat characte...      0   imdb\n",
              " 2    Attempting artiness with black & white and cle...      0   imdb\n",
              " 3         Very little music or anything to speak of.        0   imdb\n",
              " 4    The best scene in the movie was when Gerardo i...      1   imdb\n",
              " ..                                                 ...    ...    ...\n",
              " 743  I just got bored watching Jessice Lange take h...      0   imdb\n",
              " 744  Unfortunately, any virtue in this film's produ...      0   imdb\n",
              " 745                   In a word, it is embarrassing.        0   imdb\n",
              " 746                               Exceptionally bad!        0   imdb\n",
              " 747  All in all its an insult to one's intelligence...      0   imdb\n",
              " \n",
              " [748 rows x 3 columns]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "2d45133c8888889283e3c042c2e8d140a09e58ad",
        "id": "t6RqdDFRzP-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8cdc2a29-3568-4c21-894e-ccb9f24989ae"
      },
      "source": [
        "df = pd.concat(df_list)\n",
        "df.iloc[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentence    Wow... Loved this place.\n",
              "label                              1\n",
              "source                          yelp\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "1684d66bd7450cf5c0c7424f70738bcb6c902d9b",
        "id": "wB-5VWVjzP-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9ae576d9-f288-42e6-a199-d922e932f9e5"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  label source\n",
              "0                           Wow... Loved this place.      1   yelp\n",
              "1                                 Crust is not good.      0   yelp\n",
              "2          Not tasty and the texture was just nasty.      0   yelp\n",
              "3  Stopped by during the late May bank holiday of...      1   yelp\n",
              "4  The selection on the menu was great and so wer...      1   yelp"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "dea68a159b037cc68a1abe904acf9582fe4a373d",
        "id": "ObE5zp-hzP-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c8b72292-0acf-405d-f0e2-156d45086303"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>I just got bored watching Jessice Lange take h...</td>\n",
              "      <td>0</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>Unfortunately, any virtue in this film's produ...</td>\n",
              "      <td>0</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>In a word, it is embarrassing.</td>\n",
              "      <td>0</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>Exceptionally bad!</td>\n",
              "      <td>0</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>All in all its an insult to one's intelligence...</td>\n",
              "      <td>0</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sentence  label source\n",
              "743  I just got bored watching Jessice Lange take h...      0   imdb\n",
              "744  Unfortunately, any virtue in this film's produ...      0   imdb\n",
              "745                   In a word, it is embarrassing.        0   imdb\n",
              "746                               Exceptionally bad!        0   imdb\n",
              "747  All in all its an insult to one's intelligence...      0   imdb"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a90fbb437a4b0aab75956ef2be014a8a4fa81fc4",
        "id": "zIll4rTRzP-k",
        "colab_type": "text"
      },
      "source": [
        "Now use the `CountVectorizer` provided by the `scikit-learn` library to vectorize sentences. It takes the words of each sentence and creates a vocabulary of all the unique words in the sentences. This vocabulary can then be used to create a feature vector of the count of the words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cwyQ51rSzP-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5a230136-ac2d-448f-8456-7be1d44fcd82"
      },
      "source": [
        "!pip install scikit-learn"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qf-1jGCSzP-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b6090a1d36e488302b8f6fc9d1841cfc72ca0494",
        "id": "opppWzwAzP-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = ['Masha likes ice cream', 'Masha hates chocolate.']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5b1fbb71e22940fef0c4c1475a03f9c49a44450e",
        "id": "SMrgrq70zP-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8fbce9b-618b-4e5a-faf5-b83e43725a60"
      },
      "source": [
        "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
        "vectorizer.fit(sentences)\n",
        "vectorizer.vocabulary_"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Masha': 0, 'chocolate': 1, 'cream': 2, 'hates': 3, 'ice': 4, 'likes': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "a03c3dcbba4389bf4c813c90d97fc4c314cab99b",
        "id": "NnMUGjv9zP-z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cdca14d6-2cbc-4b07-aa2f-8304e77eac68"
      },
      "source": [
        "vectorizer.transform(sentences).toarray()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 1, 1],\n",
              "       [1, 1, 0, 1, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "884e3028489cb93b5e510f520dd927b2307d0efd",
        "id": "bqIBGVKGzP-2",
        "colab_type": "text"
      },
      "source": [
        "## Defining a Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fefb1c692c451e2b21e312a1d32ee219d5d1fd25",
        "id": "_i5m6pkozP-3",
        "colab_type": "text"
      },
      "source": [
        "First, you are going to split the data into a training and testing set which will allow you to evaluate the accuracy and see if your model generalizes well. This means whether the model is able to perform well on data it has not seen before. This is a way to see if the model is overfitting.\n",
        "\n",
        "**Overfitting** is when a model is trained too well on the training data. You want to avoid overfitting, as this would mean that the model mostly just memorized the training data. This would account for a large accuracy with the training data but a low accuracy in the testing data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fba1581075a612406133d1e3019520e94dd7bb7d",
        "id": "94Mw9Ct4zP-4",
        "colab_type": "text"
      },
      "source": [
        "We start by taking the Yelp data set which we extract from our concatenated data set. From there, we take the sentences and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kFX2FLGrzP-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "448e856394ba4bc54b2471cad81832c6689b0d4b",
        "id": "H9N1_v9XzP-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_yelp = df[df['source'] == 'yelp']\n",
        "sentences = df_yelp['sentence'].values\n",
        "y = df_yelp['label'].values\n",
        "\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "e1ucOvLUzP--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a6f0eef8-01c7-421e-aed2-d379228ae0c0"
      },
      "source": [
        "df_yelp.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>yelp</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  label source\n",
              "0                           Wow... Loved this place.      1   yelp\n",
              "1                                 Crust is not good.      0   yelp\n",
              "2          Not tasty and the texture was just nasty.      0   yelp\n",
              "3  Stopped by during the late May bank holiday of...      1   yelp\n",
              "4  The selection on the menu was great and so wer...      1   yelp"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wD9VPaqszP_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ebe860fb-60da-4fd0-c26c-6882c7d87bc1"
      },
      "source": [
        "print(df_yelp['label'].value_counts())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    500\n",
            "0    500\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "a47b7b5e21a856c3ebfce257f1ed483b5fe0082f",
        "id": "82BkfV8CzP_F",
        "colab_type": "text"
      },
      "source": [
        "Create the feature vectors for each sentence of the training and testing set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c5e6910c1a590924b9e8d5995656ac240287da02",
        "id": "eDVrYN1rzP_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences_train)\n",
        "\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test  = vectorizer.transform(sentences_test)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "1dc1882c7c6b68fe78729fc2625a06300cd76246",
        "id": "ufwXkEYizP_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a13de86f-76af-4dc4-9bf7-79adb759430a"
      },
      "source": [
        "print(type(X_train))\n",
        "vocab = vectorizer.vocabulary_\n",
        "print('VOCAB LENGTH = ',len(vectorizer.vocabulary_))\n",
        "\n",
        "# show 10 vocab entries\n",
        "highestCount = max(vocab.values())\n",
        "for k, v in vocab.items():\n",
        "  if v <= highestCount and v > highestCount-10 :\n",
        "    print(v,k)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "VOCAB LENGTH =  1714\n",
            "1707 you\n",
            "1709 yucky\n",
            "1708 your\n",
            "1711 yum\n",
            "1706 yet\n",
            "1712 yummy\n",
            "1713 zero\n",
            "1704 yellow\n",
            "1705 yellowtail\n",
            "1710 yukon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ca45d1d9be1a9a76ccaabff5c2dee480a0f15cb5",
        "id": "KOvnqSYHzP_M",
        "colab_type": "text"
      },
      "source": [
        "`CountVectorizer` performs tokenization which separates the sentences into a set of **tokens**. It additionally removes punctuation and special characters and can apply other preprocessing to each word. If you want, you can use a custom tokenizer from the NLTK library with the CountVectorizer or use any number of the customizations which you can explore to improve the performance of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0b9f39f8beff24e9d5cc0bd2e91d59e730e70ed8",
        "id": "1_micYN5zP_M",
        "colab_type": "text"
      },
      "source": [
        "The classification model we are going to use is the logistic regression which is a simple yet powerful linear model that is mathematically speaking in fact a form of regression between 0 and 1 based on the input feature vector. By specifying a cutoff value (by default 0.5), the regression model is used for classification. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bPbZHBYpzP_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "db2f442632a368f7c0520dc795b80e956f26032f",
        "id": "Ht4llHHqzP_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8632036-ba30-4cac-fe8f-7ca012334423"
      },
      "source": [
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0b534cd4551329ef0ffcaa2b13f0336d86dd6025",
        "id": "Z5ftQOqzzP_V",
        "colab_type": "text"
      },
      "source": [
        "You can see that the logistic regression reached an impressive 79.6%, but let’s have a look how this model performs on the other data sets that we have. In this script, we perform and evaluate the whole process for each data set that we have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "2612ace5fb3fc3a15f1619ae6f7a12895821cd9d",
        "id": "ZyQs1o9vzP_W",
        "colab_type": "code",
        "colab": {},
        "outputId": "9d66add0-9fb4-478e-9d5e-6dd3c3e5aa25"
      },
      "source": [
        "for source in df['source'].unique():\n",
        "    df_source = df[df['source'] == source]\n",
        "    sentences = df_source['sentence'].values\n",
        "    y = df_source['label'].values\n",
        "\n",
        "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "        sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(sentences_train)\n",
        "    X_train = vectorizer.transform(sentences_train)\n",
        "    X_test  = vectorizer.transform(sentences_test)\n",
        "\n",
        "    classifier = LogisticRegression()\n",
        "    classifier.fit(X_train, y_train)\n",
        "    score = classifier.score(X_test, y_test)\n",
        "    print('Accuracy for {} data: {:.4f}'.format(source, score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for yelp data: 0.7960\n",
            "Accuracy for amazon data: 0.7960\n",
            "Accuracy for imdb data: 0.7487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "40150518b8c2f350daeac8043a489c237111431b",
        "id": "VeJ4kvhAzP_Y",
        "colab_type": "text"
      },
      "source": [
        "Great! You can see that this fairly simple model achieves a fairly good accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fd98829d49fdd69dbd9748c9767cbf4de01ddbd5",
        "id": "idtLaBrRzP_Z",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to Deep Neural Networks\n",
        "**Neural networks**, or sometimes called **artificial neural network (ANN)** or**feedforward neural network**, are computational networks which were vaguely inspired by the neural networks in the human brain. They consist of neurons (also called nodes) which are connected like in the graph below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "45af00ca1e9cdedd560eba5dde52cba51fdd4767",
        "id": "ViEPKxCZzP_Z",
        "colab_type": "text"
      },
      "source": [
        "You start by having a layer of input neurons where you feed in your feature vectors and the values are then feeded forward to a hidden layer. At each connection, you are feeding the value forward, while the value is multiplied by a weight and a bias is added to the value. This happens at every connection and at the end you reach an output layer with one or more output nodes.\n",
        "\n",
        "If you want to have a binary classification you can use one node, but if you have multiple categories you should use multiple nodes for each category:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b1220c0b5e7bb8fb1dd582634f90766eb170a339",
        "id": "qp6VuDgOzP_c",
        "colab_type": "text"
      },
      "source": [
        "### Introducing Keras\n",
        "\n",
        "Keras is a deep learning and neural networks API by François Chollet which is capable of running on top of Tensorflow (Google), Theano or CNTK (Microsoft). To quote the wonderful book by François Chollet, Deep Learning with Python:\n",
        "\n",
        "    Keras is a model-level library, providing high-level building blocks for developing deep-learning models. It doesn’t handle low-level operations such as tensor manipulation and differentiation. Instead, it relies on a specialized, well-optimized tensor library to do so, serving as the backend engine of Keras (Source)\n",
        "\n",
        "It is a great way to start experimenting with neural networks without having to implement every layer and piece on your own. For example Tensorflow is a great machine learning library, but you have to implement a lot of boilerplate code to have a model running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "20a29d7bf57fb7d112268df29808183e552ec677",
        "id": "R9PRFJ3LzP_c",
        "colab_type": "text"
      },
      "source": [
        "### First Keras Model\n",
        "\n",
        "Keras supports two main types of models. You have the Sequential model API and the functional API which can do everything of the Sequential model but it can be also used for advanced models with complex network architectures.\n",
        "\n",
        "The Sequential model is a linear stack of layers, where you can use the large variety of available layers in Keras. The most common layer is the Dense layer which is your regular densely connected neural network layer with all the weights and biases that you are already familiar with.\n",
        "\n",
        "Before we build our model, we need to know the input dimension of our feature vectors. This happens only in the first layer since the following layers can do automatic shape inference. In order to build the Sequential model, you can add layers one by one in order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "I0ScHqzPzP_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "aac9e9b76e57a9fb86ddb47641c0b4162cb187a9",
        "id": "QRya99SbzP_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = X_train.shape[1]  # Number of features\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ce727fbc7435f8067d25a941f62ddb9104b61b99",
        "id": "F0bg4E6LzP_l",
        "colab_type": "code",
        "colab": {},
        "outputId": "4abb53d5-82a4-4006-8258-655be3041b82"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 10)                25060     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 25,071\n",
            "Trainable params: 25,071\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "edebe70a32227f73b9ddd91e5f54691f7571ee82",
        "id": "9UM_QIJszP_o",
        "colab_type": "code",
        "colab": {},
        "outputId": "fa4d7c92-a1f5-40bd-ce7a-bfe872d1e5c8"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 561 samples, validate on 187 samples\n",
            "Epoch 1/100\n",
            "561/561 [==============================] - 1s 1ms/step - loss: 0.6925 - acc: 0.5330 - val_loss: 0.6844 - val_acc: 0.6364\n",
            "Epoch 2/100\n",
            "561/561 [==============================] - 0s 340us/step - loss: 0.6488 - acc: 0.7897 - val_loss: 0.6635 - val_acc: 0.6845\n",
            "Epoch 3/100\n",
            "561/561 [==============================] - 0s 252us/step - loss: 0.5840 - acc: 0.8859 - val_loss: 0.6280 - val_acc: 0.7540\n",
            "Epoch 4/100\n",
            "561/561 [==============================] - 0s 252us/step - loss: 0.4925 - acc: 0.9287 - val_loss: 0.6005 - val_acc: 0.7647\n",
            "Epoch 5/100\n",
            "561/561 [==============================] - 0s 258us/step - loss: 0.3947 - acc: 0.9608 - val_loss: 0.5766 - val_acc: 0.7647\n",
            "Epoch 6/100\n",
            "561/561 [==============================] - 0s 256us/step - loss: 0.3130 - acc: 0.9733 - val_loss: 0.5336 - val_acc: 0.7754\n",
            "Epoch 7/100\n",
            "561/561 [==============================] - 0s 268us/step - loss: 0.2512 - acc: 0.9804 - val_loss: 0.5254 - val_acc: 0.7807\n",
            "Epoch 8/100\n",
            "561/561 [==============================] - 0s 256us/step - loss: 0.2034 - acc: 0.9804 - val_loss: 0.4831 - val_acc: 0.7754\n",
            "Epoch 9/100\n",
            "561/561 [==============================] - 0s 257us/step - loss: 0.1685 - acc: 0.9875 - val_loss: 0.5114 - val_acc: 0.7807\n",
            "Epoch 10/100\n",
            "561/561 [==============================] - 0s 250us/step - loss: 0.1411 - acc: 0.9875 - val_loss: 0.4785 - val_acc: 0.7968\n",
            "Epoch 11/100\n",
            "561/561 [==============================] - 0s 254us/step - loss: 0.1196 - acc: 0.9929 - val_loss: 0.4814 - val_acc: 0.7968\n",
            "Epoch 12/100\n",
            "561/561 [==============================] - 0s 250us/step - loss: 0.1022 - acc: 0.9964 - val_loss: 0.4770 - val_acc: 0.7968\n",
            "Epoch 13/100\n",
            "561/561 [==============================] - 0s 251us/step - loss: 0.0881 - acc: 0.9982 - val_loss: 0.4767 - val_acc: 0.8075\n",
            "Epoch 14/100\n",
            "561/561 [==============================] - 0s 255us/step - loss: 0.0770 - acc: 0.9982 - val_loss: 0.4891 - val_acc: 0.8075\n",
            "Epoch 15/100\n",
            "561/561 [==============================] - 0s 255us/step - loss: 0.0674 - acc: 0.9982 - val_loss: 0.4776 - val_acc: 0.8128\n",
            "Epoch 16/100\n",
            "561/561 [==============================] - 0s 249us/step - loss: 0.0597 - acc: 0.9982 - val_loss: 0.4833 - val_acc: 0.8128\n",
            "Epoch 17/100\n",
            "561/561 [==============================] - 0s 251us/step - loss: 0.0531 - acc: 0.9982 - val_loss: 0.4829 - val_acc: 0.8075\n",
            "Epoch 18/100\n",
            "561/561 [==============================] - 0s 256us/step - loss: 0.0474 - acc: 0.9982 - val_loss: 0.5095 - val_acc: 0.7968\n",
            "Epoch 19/100\n",
            "561/561 [==============================] - 0s 254us/step - loss: 0.0426 - acc: 0.9982 - val_loss: 0.5057 - val_acc: 0.7914\n",
            "Epoch 20/100\n",
            "561/561 [==============================] - 0s 250us/step - loss: 0.0383 - acc: 0.9982 - val_loss: 0.5138 - val_acc: 0.7861\n",
            "Epoch 21/100\n",
            "561/561 [==============================] - 0s 250us/step - loss: 0.0348 - acc: 0.9982 - val_loss: 0.5166 - val_acc: 0.7914\n",
            "Epoch 22/100\n",
            "561/561 [==============================] - 0s 245us/step - loss: 0.0315 - acc: 0.9982 - val_loss: 0.5257 - val_acc: 0.7914\n",
            "Epoch 23/100\n",
            "561/561 [==============================] - 0s 247us/step - loss: 0.0289 - acc: 0.9982 - val_loss: 0.5486 - val_acc: 0.7914\n",
            "Epoch 24/100\n",
            "561/561 [==============================] - 0s 256us/step - loss: 0.0262 - acc: 0.9982 - val_loss: 0.5400 - val_acc: 0.7861\n",
            "Epoch 25/100\n",
            "561/561 [==============================] - 0s 250us/step - loss: 0.0240 - acc: 0.9982 - val_loss: 0.5526 - val_acc: 0.7861\n",
            "Epoch 26/100\n",
            "561/561 [==============================] - 0s 262us/step - loss: 0.0220 - acc: 0.9982 - val_loss: 0.5551 - val_acc: 0.7807\n",
            "Epoch 27/100\n",
            "561/561 [==============================] - 0s 253us/step - loss: 0.0203 - acc: 0.9982 - val_loss: 0.5601 - val_acc: 0.7807\n",
            "Epoch 28/100\n",
            "561/561 [==============================] - 0s 247us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.5788 - val_acc: 0.7807\n",
            "Epoch 29/100\n",
            "561/561 [==============================] - 0s 247us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.5812 - val_acc: 0.7807\n",
            "Epoch 30/100\n",
            "561/561 [==============================] - 0s 254us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.5898 - val_acc: 0.7807\n",
            "Epoch 31/100\n",
            "561/561 [==============================] - 0s 251us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.5892 - val_acc: 0.7861\n",
            "Epoch 32/100\n",
            "561/561 [==============================] - 0s 252us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.6025 - val_acc: 0.7861\n",
            "Epoch 33/100\n",
            "561/561 [==============================] - 0s 251us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.6135 - val_acc: 0.7861\n",
            "Epoch 34/100\n",
            "561/561 [==============================] - 0s 254us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.6099 - val_acc: 0.7861\n",
            "Epoch 35/100\n",
            "561/561 [==============================] - 0s 251us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.6154 - val_acc: 0.7861\n",
            "Epoch 36/100\n",
            "561/561 [==============================] - 0s 266us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.6304 - val_acc: 0.7861\n",
            "Epoch 37/100\n",
            "561/561 [==============================] - 0s 321us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.6380 - val_acc: 0.7861\n",
            "Epoch 38/100\n",
            "561/561 [==============================] - 0s 249us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.6402 - val_acc: 0.7861\n",
            "Epoch 39/100\n",
            "561/561 [==============================] - 0s 256us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.6563 - val_acc: 0.7861\n",
            "Epoch 40/100\n",
            "561/561 [==============================] - 0s 251us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.6482 - val_acc: 0.7861\n",
            "Epoch 41/100\n",
            "561/561 [==============================] - 0s 264us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.6606 - val_acc: 0.7807\n",
            "Epoch 42/100\n",
            "561/561 [==============================] - 0s 258us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.6691 - val_acc: 0.7807\n",
            "Epoch 43/100\n",
            "561/561 [==============================] - 0s 246us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.6672 - val_acc: 0.7807\n",
            "Epoch 44/100\n",
            "561/561 [==============================] - 0s 257us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.6779 - val_acc: 0.7807\n",
            "Epoch 45/100\n",
            "561/561 [==============================] - 0s 248us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.6872 - val_acc: 0.7807\n",
            "Epoch 46/100\n",
            "561/561 [==============================] - 0s 253us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.7016 - val_acc: 0.7807\n",
            "Epoch 47/100\n",
            "561/561 [==============================] - 0s 254us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.6990 - val_acc: 0.7807\n",
            "Epoch 48/100\n",
            "561/561 [==============================] - 0s 251us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.7137 - val_acc: 0.7807\n",
            "Epoch 49/100\n",
            "561/561 [==============================] - 0s 254us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.7101 - val_acc: 0.7807\n",
            "Epoch 50/100\n",
            "561/561 [==============================] - 0s 253us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.7189 - val_acc: 0.7807\n",
            "Epoch 51/100\n",
            "561/561 [==============================] - 0s 255us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.7300 - val_acc: 0.7754\n",
            "Epoch 52/100\n",
            "561/561 [==============================] - 0s 252us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.7308 - val_acc: 0.7754\n",
            "Epoch 53/100\n",
            "561/561 [==============================] - 0s 256us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.7368 - val_acc: 0.7754\n",
            "Epoch 54/100\n",
            "561/561 [==============================] - 0s 254us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.7423 - val_acc: 0.7754\n",
            "Epoch 55/100\n",
            "561/561 [==============================] - 0s 254us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.7578 - val_acc: 0.7754\n",
            "Epoch 56/100\n",
            "561/561 [==============================] - 0s 251us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.7588 - val_acc: 0.7754\n",
            "Epoch 57/100\n",
            "561/561 [==============================] - 0s 252us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.7671 - val_acc: 0.7754\n",
            "Epoch 58/100\n",
            "561/561 [==============================] - 0s 256us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.7725 - val_acc: 0.7754\n",
            "Epoch 59/100\n",
            "561/561 [==============================] - 0s 258us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.7775 - val_acc: 0.7754\n",
            "Epoch 60/100\n",
            "561/561 [==============================] - 0s 265us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.7826 - val_acc: 0.7754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 61/100\n",
            "561/561 [==============================] - 0s 264us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.7883 - val_acc: 0.7754\n",
            "Epoch 62/100\n",
            "561/561 [==============================] - 0s 277us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.7927 - val_acc: 0.7754\n",
            "Epoch 63/100\n",
            "561/561 [==============================] - 0s 271us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.7974 - val_acc: 0.7754\n",
            "Epoch 64/100\n",
            "561/561 [==============================] - 0s 274us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.8032 - val_acc: 0.7754\n",
            "Epoch 65/100\n",
            "561/561 [==============================] - 0s 321us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.8089 - val_acc: 0.7754\n",
            "Epoch 66/100\n",
            "561/561 [==============================] - 0s 340us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.8122 - val_acc: 0.7754\n",
            "Epoch 67/100\n",
            "561/561 [==============================] - 0s 293us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.8177 - val_acc: 0.7754\n",
            "Epoch 68/100\n",
            "561/561 [==============================] - 0s 282us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.8220 - val_acc: 0.7754\n",
            "Epoch 69/100\n",
            "561/561 [==============================] - 0s 290us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.8262 - val_acc: 0.7754\n",
            "Epoch 70/100\n",
            "561/561 [==============================] - 0s 272us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.8268 - val_acc: 0.7754\n",
            "Epoch 71/100\n",
            "561/561 [==============================] - 0s 273us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.8346 - val_acc: 0.7754\n",
            "Epoch 72/100\n",
            "561/561 [==============================] - 0s 277us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.8394 - val_acc: 0.7754\n",
            "Epoch 73/100\n",
            "561/561 [==============================] - 0s 288us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.8441 - val_acc: 0.7754\n",
            "Epoch 74/100\n",
            "561/561 [==============================] - 0s 283us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.8482 - val_acc: 0.7754\n",
            "Epoch 75/100\n",
            "561/561 [==============================] - 0s 291us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.8534 - val_acc: 0.7754\n",
            "Epoch 76/100\n",
            "561/561 [==============================] - 0s 292us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.8581 - val_acc: 0.7754\n",
            "Epoch 77/100\n",
            "561/561 [==============================] - 0s 401us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.8624 - val_acc: 0.7754\n",
            "Epoch 78/100\n",
            "561/561 [==============================] - 0s 406us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.8663 - val_acc: 0.7754\n",
            "Epoch 79/100\n",
            "561/561 [==============================] - 0s 416us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.8712 - val_acc: 0.7754\n",
            "Epoch 80/100\n",
            "561/561 [==============================] - 0s 392us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.8763 - val_acc: 0.7754\n",
            "Epoch 81/100\n",
            "561/561 [==============================] - 0s 340us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.8809 - val_acc: 0.7754\n",
            "Epoch 82/100\n",
            "561/561 [==============================] - 0s 276us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.8849 - val_acc: 0.7754\n",
            "Epoch 83/100\n",
            "561/561 [==============================] - 0s 274us/step - loss: 9.9746e-04 - acc: 1.0000 - val_loss: 0.8887 - val_acc: 0.7754\n",
            "Epoch 84/100\n",
            "561/561 [==============================] - 0s 274us/step - loss: 9.5876e-04 - acc: 1.0000 - val_loss: 0.8940 - val_acc: 0.7807\n",
            "Epoch 85/100\n",
            "561/561 [==============================] - 0s 279us/step - loss: 9.2249e-04 - acc: 1.0000 - val_loss: 0.8983 - val_acc: 0.7754\n",
            "Epoch 86/100\n",
            "561/561 [==============================] - 0s 273us/step - loss: 8.8466e-04 - acc: 1.0000 - val_loss: 0.9022 - val_acc: 0.7754\n",
            "Epoch 87/100\n",
            "561/561 [==============================] - 0s 269us/step - loss: 8.5354e-04 - acc: 1.0000 - val_loss: 0.9064 - val_acc: 0.7754\n",
            "Epoch 88/100\n",
            "561/561 [==============================] - 0s 282us/step - loss: 8.2169e-04 - acc: 1.0000 - val_loss: 0.9113 - val_acc: 0.7754\n",
            "Epoch 89/100\n",
            "561/561 [==============================] - 0s 293us/step - loss: 7.8654e-04 - acc: 1.0000 - val_loss: 0.9160 - val_acc: 0.7754\n",
            "Epoch 90/100\n",
            "561/561 [==============================] - 0s 288us/step - loss: 7.5810e-04 - acc: 1.0000 - val_loss: 0.9198 - val_acc: 0.7754\n",
            "Epoch 91/100\n",
            "561/561 [==============================] - 0s 273us/step - loss: 7.2915e-04 - acc: 1.0000 - val_loss: 0.9240 - val_acc: 0.7754\n",
            "Epoch 92/100\n",
            "561/561 [==============================] - 0s 278us/step - loss: 7.0288e-04 - acc: 1.0000 - val_loss: 0.9292 - val_acc: 0.7754\n",
            "Epoch 93/100\n",
            "561/561 [==============================] - 0s 279us/step - loss: 6.7629e-04 - acc: 1.0000 - val_loss: 0.9327 - val_acc: 0.7754\n",
            "Epoch 94/100\n",
            "561/561 [==============================] - 0s 273us/step - loss: 6.5318e-04 - acc: 1.0000 - val_loss: 0.9376 - val_acc: 0.7754\n",
            "Epoch 95/100\n",
            "561/561 [==============================] - 0s 271us/step - loss: 6.2698e-04 - acc: 1.0000 - val_loss: 0.9417 - val_acc: 0.7754\n",
            "Epoch 96/100\n",
            "561/561 [==============================] - 0s 282us/step - loss: 6.0390e-04 - acc: 1.0000 - val_loss: 0.9477 - val_acc: 0.7754\n",
            "Epoch 97/100\n",
            "561/561 [==============================] - 0s 292us/step - loss: 5.8129e-04 - acc: 1.0000 - val_loss: 0.9519 - val_acc: 0.7754\n",
            "Epoch 98/100\n",
            "561/561 [==============================] - 0s 279us/step - loss: 5.5907e-04 - acc: 1.0000 - val_loss: 0.9555 - val_acc: 0.7754\n",
            "Epoch 99/100\n",
            "561/561 [==============================] - 0s 278us/step - loss: 5.3995e-04 - acc: 1.0000 - val_loss: 0.9596 - val_acc: 0.7754\n",
            "Epoch 100/100\n",
            "561/561 [==============================] - 0s 276us/step - loss: 5.2070e-04 - acc: 1.0000 - val_loss: 0.9640 - val_acc: 0.7754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e8569a049fd01db63a8e36554a4855be05aa7295",
        "id": "0y23jofUzP_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "a4a3fc45fe1b3dc344ff30dd5c1982d9734bfe4e",
        "id": "nwYryP7rzP_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "bcc0bc67371a228af0bebb22b9b95dcd5276c58e",
        "id": "0ZiIZHhizP_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "ea9e401688489ddccdd8b028bc962101d1f50d0c",
        "id": "AYh1JpMrzP_y",
        "colab_type": "text"
      },
      "source": [
        "## What Is a Word Embedding?\n",
        "\n",
        "Text is considered a form of sequence data similar to time series data that you would have in weather data or financial data. Now you will see how to represent each word as vectors. There are various ways to vectorize text, such as:\n",
        "\n",
        "* Words represented by each word as a vector\n",
        "* Characters represented by each character as a vector\n",
        "* N-grams of words/characters represented as a vector (N-grams are overlapping groups of multiple succeeding words/characters in the text)\n",
        "\n",
        "Here, you’ll see how to deal with representing words as vectors which is the common way to use text in neural networks. Two possible ways to represent a word as a vector are one-hot encoding and word embeddings.\n",
        "\n",
        "**One-Hot Encoding**\n",
        "\n",
        "The first way to represent a word as a vector is by creating a so-called one-hot encoding, which is simply done by taking a vector of the length of the vocabulary with an entry for each word in the corpus.\n",
        "\n",
        "In this way, you have for each word, given it has a spot in the vocabulary, a vector with zeros everywhere except for the corresponding spot for the word which is set to one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e70cf30124b97e5354817229035e448717599353",
        "id": "XjgkJehSzP_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cities = ['London', 'Berlin', 'Berlin', 'New York', 'London']\n",
        "cities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fa32a60f2a6f313ea8bc440942f2d489998b6a0c",
        "id": "caexiUyjzP_2",
        "colab_type": "text"
      },
      "source": [
        "`LabelEncoder` to encode the list of cities into categorical integer values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "3d4ecdc38e764ecfeb1f0d78a16ecf423983cdb9",
        "id": "JWkUCz1dzP_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "city_labels = encoder.fit_transform(cities)\n",
        "city_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "da045f493813e22a4bf274d8e68a7adbcaa39cf8",
        "id": "9eGIh2plzP_5",
        "colab_type": "text"
      },
      "source": [
        "OneHotEncoder expects each categorical value to be in a separate row, so you’ll need to reshape the array, then you can apply the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "a811470d725cfc3edacc04cfb8ff40050408b93c",
        "id": "tS_Qh7x2zP_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = OneHotEncoder(sparse=False)\n",
        "city_labels = city_labels.reshape((5, 1))\n",
        "encoder.fit_transform(city_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9005576faab272d4533590b721b525e726b9b4a4",
        "id": "NBbTJpuazP_8",
        "colab_type": "text"
      },
      "source": [
        "**Word Embeddings**\n",
        "\n",
        "This method represents words as dense word vectors (also called word embeddings) which are trained unlike the one-hot encoding which are hardcoded. This means that the word embeddings collect more information into fewer dimensions.\n",
        "\n",
        "Note that the word embeddings do not understand the text as a human would, but they rather map the statistical structure of the language used in the corpus. Their aim is to map semantic meaning into a geometric space. This geometric space is then called the embedding space.\n",
        "\n",
        "Now you need to tokenize the data into a format that can be used by the word embeddings. Keras offers a couple of convenience methods for text preprocessing and sequence preprocessing which you can employ to prepare your text.\n",
        "\n",
        "You can start by using the Tokenizer utility class which can vectorize a text corpus into a list of integers. Each integer maps to a value in a dictionary that encodes the entire corpus, with the keys in the dictionary being the vocabulary terms themselves. You can add the parameter num_words, which is responsible for setting the size of the vocabulary. The most common num_words words will be then kept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "19df1b964d253c06f51e5392b2385bb97e70ed88",
        "id": "B81DurbdzP_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(sentences_train[2])\n",
        "print(X_train[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "a7f9e7a00eb777752eb95bfac883bd9e2eee74ca",
        "id": "BXsX7Z5VzQAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in ['the', 'all','fan']:\n",
        "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e5f5896fe077325548dd5f6a59f6e806c0d5da23",
        "id": "3_rwjK2hzQAI",
        "colab_type": "text"
      },
      "source": [
        "** pad sequences with Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "f76a6270c28c045567f1db6525034c72669dcc06",
        "id": "m5F6-XPizQAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 100\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train[0, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "73f8ef7851a32479c2d932d33c4c609e21f7cd33",
        "id": "dXaL0j60zQAM",
        "colab_type": "text"
      },
      "source": [
        "### Keras Embedding Layer\n",
        "\n",
        "Now you can use the Embedding Layer of Keras which takes the previously calculated integers and maps them to a dense vector of the embedding. You will need the following parameters:\n",
        "* **input_dim**: the size of the vocabulary\n",
        "* **output_dim**: the size of the dense vector\n",
        "* **input_length**: the length of the sequence\n",
        "\n",
        "With the Embedding layer we have now a couple of options. One way would be to take the output of the embedding layer and plug it into a Dense layer. In order to do this you have to add a Flatten layer in between that prepares the sequential input for the Dense layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0294cc68d4f93cd6bd3cf0080309cc7047f4daec",
        "id": "NZOJ41T7zQAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "046c27a78bb27f05cd3f731ae9fb6fbfabf471bc",
        "id": "zhousu3AzQAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "80959ffdd0007815cf8029424a3ac9a99b6ea0f3",
        "id": "g7rVZ2BrzQAU",
        "colab_type": "text"
      },
      "source": [
        "Global max/average pooling takes the maximum/average of all features whereas in the other case you have to define the pool size. Keras has again its own layer that you can add in the sequential model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "4e6fe427c11f4ef8b93b00a4ddd0543f955437a3",
        "id": "J1QLGELOzQAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "f006794931241f2e77093d719252ee1dfc94828b",
        "id": "tuhYRQf_zQAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "53b4098556381b3b10f16043673e27823ab64153",
        "id": "mOnBO1hXzQAd",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Neural Networks (CNN)\n",
        "\n",
        "Convolutional neural networks or also called **convnets** are one of the most exciting developments in machine learning in recent years.\n",
        "\n",
        "They have revolutionized image classification and computer vision by being able to extract features from images and using them in neural networks. The properties that made them useful in image processing makes them also handy for sequence processing. You can imagine a CNN as a specialized neural network that is able to detect specific patterns.\n",
        "\n",
        "A CNN has hidden layers which are called convolutional layers. When you think of images, a computer has to deal with a two dimensional matrix of numbers and therefore you need some way to detect features in this matrix. These convolutional layers are able to detect edges, corners and other kinds of textures which makes them such a special tool. The convolutional layer consists of multiple filters which are slid across the image and are able to detect specific features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "2f8b18e5bee409b3da521379c3bb6c6c8a53c075",
        "id": "AD_9uKaQzQAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "8298fb93c84b464ff93e1a0a54a085faacec5ebf",
        "id": "xjs9fBA2zQAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "985360d558b0fcd6b6f17bd8f7599a091b5aa35c",
        "id": "P9pZGRBqzQAl",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters Optimization\n",
        "\n",
        "One crucial steps of deep learning and working with neural networks is hyperparameter optimization.\n",
        "\n",
        "As you saw in the models that we have used so far, even with simpler ones, you had a large number of parameters to tweak and choose from. Those parameters are called hyperparameters. This is the most time consuming part of machine learning and sadly there are no one-fits-all solutions ready.\n",
        "\n",
        "One popular method for hyperparameter optimization is **grid search**. What this method does is it takes lists of parameters and it runs the model with each parameter combination that it can find. It is the most thorough way but also the most computationally heavy way to do this. Another common way,**random search**, which you’ll see in action here, simply takes random combinations of parameters.\n",
        "\n",
        "In order to apply random search with Keras, you will need to use the KerasClassifier which serves as a wrapper for the scikit-learn API. With this wrapper you are able to use the various tools available with scikit-learn like cross-validation. The class that you need is RandomizedSearchCV which implements random search with cross-validation. Cross-validation is a way to validate the model and take the whole data set and separate it into multiple testing and training data sets.\n",
        "\n",
        "There are various types of cross-validation. One type is the k-fold cross-validation. In this type the data set is partitioned into k equal sized sets where one set is used for testing and the rest of the partitions are used for training. This enables you to run k different runs, where each partition is once used as a testing set. So, the higher k is the more accurate the model evaluation is, but the smaller each testing set is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b347dfa060395fafe259a56d4bf4385d213a6727",
        "id": "Izzp57oezQAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cbdba00bf86fe148d34f58051aa8b74aa410894f",
        "id": "Nx3V5rIJzQAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[5000], \n",
        "                  embedding_dim=[50],\n",
        "                  maxlen=[100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "a33d1066e49c58b8305208f5a59d220f7097bcb8",
        "id": "aKBQK8aRzQAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 50\n",
        "maxlen = 100\n",
        "output_file = 'output.txt'\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "for source, frame in df.groupby('source'):\n",
        "    print('Running grid search for data set :', source)\n",
        "    sentences = df['sentence'].values\n",
        "    y = df['label'].values\n",
        "\n",
        "    # Train-test split\n",
        "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "        sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    # Tokenize words\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(sentences_train)\n",
        "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "    # Adding 1 because of reserved 0 index\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Pad sequences with zeros\n",
        "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "    # Parameter grid for grid search\n",
        "    param_grid = dict(num_filters=[32, 64, 128],\n",
        "                      kernel_size=[3, 5, 7],\n",
        "                      vocab_size=[vocab_size],\n",
        "                      embedding_dim=[embedding_dim],\n",
        "                      maxlen=[maxlen])\n",
        "    model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=10,\n",
        "                            verbose=False)\n",
        "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                              cv=4, verbose=1, n_iter=5)\n",
        "    grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate testing set\n",
        "    test_accuracy = grid.score(X_test, y_test)\n",
        "\n",
        "    # Save and evaluate results\n",
        "#     prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n",
        "#     if prompt.lower() not in {'y', 'true', 'yes'}:\n",
        "#         break\n",
        "#     with open(output_file, 'w+') as f:\n",
        "    s = ('Running {} data set\\nBest Accuracy : '\n",
        "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
        "    output_string = s.format(\n",
        "        source,\n",
        "        grid_result.best_score_,\n",
        "        grid_result.best_params_,\n",
        "        test_accuracy)\n",
        "    print(output_string)\n",
        "#         f.write(output_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "87d72394ce27585385eddc427b555203160cb33c",
        "id": "tXvvFTCLzQA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}